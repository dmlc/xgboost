/*
 Copyright (c) 2014 by Contributors

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 */

package ml.dmlc.xgboost4j.scala.spark.rapids

import scala.collection.JavaConverters._

import ml.dmlc.xgboost4j.java.Rabit
import org.apache.commons.logging.LogFactory

import org.apache.spark.sql.{DataFrame, Dataset, Row}
import ml.dmlc.xgboost4j.scala.{Booster, DMatrix}
import ml.dmlc.xgboost4j.scala.spark._

import org.apache.spark.TaskContext
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.sql.types.{ArrayType, FloatType, StringType, StructType}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.expressions.UnsafeProjection
import org.apache.spark.sql.vectorized.ColumnarBatch
import scala.collection.Iterator

import ml.dmlc.xgboost4j.gpu.java.CudfColumnBatch
import ml.dmlc.xgboost4j.java.spark.{GpuColumnBatch, GpuColumnVector}

private[spark] object GpuTransform {

  private val logger = LogFactory.getLog("GpuTransform")

  // ==== API for xgboost classifier plays the same functionality with CPU transformInternal
  def transformInternal(tran: XGBoostClassificationModel, dataset: Dataset[_],
      sampler: Option[GpuSampler]): DataFrame = {
    // The names of the columns generated by classification model after transforming.
    // Should keep the same order with "colDataItrs" below.
    val Seq(leafName, contribName) = MLUtils.getColumnNames(tran)(
      tran.leafPredictionCol,
      tran.contribPredictionCol
    )
    val colNames = Seq(
      XGBoostClassificationModel._rawPredictionCol,
      XGBoostClassificationModel._probabilityCol,
      leafName, contribName)

    // predictFunc = producePredictionItrs + produceResultIterator
    val predictFunc = (bBooster: Broadcast[Booster], dm: DMatrix, rawIter: Iterator[Row]) => {
      val colDataItrs = tran.producePredictionItrs(bBooster, dm)
      buildRddIterator(colNames, colDataItrs, rawIter)
    }
    transformDistributed(dataset, tran.nativeBooster, tran.getMissing, tran.getFeaturesCols,
      tran.getToRowCols, predictFunc, buildRddSchema(colNames),
      tran.getBuildAllColumnsInTransform, sampler)
  }

  // ==== API for xgboost regressor plays the same functionality with CPU transformInternal
  def transformInternal(tran: XGBoostRegressionModel, dataset: Dataset[_],
        validateCols: Boolean, sampler: Option[GpuSampler]): DataFrame = {
    val castedDF = if (validateCols) {
      MLUtils.prepareColumnType(dataset, tran.getFeaturesCols, fitting = false)
    } else {
      dataset
    }
    transformInternal(tran, castedDF, sampler)
  }

  private def transformInternal(
      tran: XGBoostRegressionModel,
      dataset: Dataset[_],
      sampler: Option[GpuSampler]): DataFrame = {
    // The names of the columns generated by classification model after transforming.
    // Should keep the same order with "colDataItrs" below.
    val Seq(leafName, contribName) = MLUtils.getColumnNames(tran)(
      tran.leafPredictionCol,
      tran.contribPredictionCol
    )
    val colNames = Seq(XGBoostRegressionModel._originalPredictionCol, leafName, contribName)

    val predictFunc = (bBooster: Broadcast[Booster], dm: DMatrix, rawIter: Iterator[Row]) => {
      val colDataItrs = tran.producePredictionItrs(bBooster, dm)
      buildRddIterator(colNames, colDataItrs, rawIter)
    }
    transformDistributed(dataset, tran.nativeBooster, tran.getMissing, tran.getFeaturesCols,
      tran.getToRowCols, predictFunc, buildRddSchema(colNames),
      tran.getBuildAllColumnsInTransform, sampler)
  }

  private def buildRddSchema(colNames: Seq[String])(rawSchema: StructType): StructType = {
    var rddSchema = rawSchema
    val dataType = ArrayType(FloatType, containsNull = false)
    colNames.foreach { name =>
      if (name.nonEmpty) rddSchema = rddSchema.add(name, dataType, false)
    }
    rddSchema
  }

  private def buildRddIterator(colNames: Seq[String],
      colDataItrs: Seq[Iterator[Row]],
      rawIter: Iterator[Row]): Iterator[Row] = {
    require(colNames.length == colDataItrs.length)
    colNames.zip(colDataItrs).foldLeft(rawIter) {
      case (outIt, (cName, dataIt)) =>
        if (cName.nonEmpty && dataIt.nonEmpty) {
          outIt.zip(dataIt).map {
            case (origRow, dataRow) => Row.fromSeq(origRow.toSeq ++ dataRow.toSeq)
          }
        } else outIt
    }
  }

  private def transformDistributed(ds: Dataset[_],
      booster: Booster,
      missing: Float,
      featureColNames: Seq[String],
      toRowColNames: Seq[String],
      predictFunc: (Broadcast[Booster], DMatrix, Iterator[Row]) => Iterator[Row],
      buildRddSchema: StructType => StructType,
      buildAllColumns: Boolean = true,
      sampler: Option[GpuSampler] = None): DataFrame = {

    logger.info(s"Running GPU transfrom with buildAllColumns:${buildAllColumns}" +
      s" column:${toRowColNames}")

    // Column check supposed to be done before getting here
    val origSchema = ds.schema

    val rowSchema = ColumnBatchToRow.buildRowSchema(origSchema, toRowColNames, buildAllColumns)
    val sc = ds.sparkSession.sparkContext
    // Prepare some vars will be passed to executors.
    val bOrigSchema = sc.broadcast(origSchema)
    val bRowSchema = sc.broadcast(rowSchema)
    val bBooster = sc.broadcast(booster)

    // Small vars so don't need to broadcast them
    val isLocal = sc.isLocal
    val featureIds = featureColNames.distinct.map(origSchema.fieldIndex)

    // start transform by df->rd->mapPartition
    val rowRDD: RDD[Row] = GpuUtils.toColumnarRdd(ds.asInstanceOf[DataFrame]).mapPartitions {
      iter =>

        if (buildAllColumns && sampler.isEmpty) {
          // UnsafeProjection is not serializable so do it on the executor side
          val toUnsafe = UnsafeProjection.create(bOrigSchema.value)
          new Iterator[Row] {
            private var batchCnt = 0
            private var converter: RowConverter = null

            // GPU batches read in must be closed by the receiver (us)
            @transient var cb: ColumnarBatch = null
            var it: Iterator[Row] = null

            TaskContext.get().addTaskCompletionListener[Unit](_ => {
              if (batchCnt > 0) {
                Rabit.shutdown()
              }
              closeCurrentBatch()
            })

            private def closeCurrentBatch(): Unit = {
              if (cb != null) {
                cb.close()
                cb = null
              }
            }

            def loadNextBatch(): Unit = {
              closeCurrentBatch()
              if (it != null) {
                it = null
              }
              if (iter.hasNext) {
                val table = iter.next()

                if (batchCnt == 0) {
                  // do we really need to involve rabit in transform?
                  // Init rabit
                  val rabitEnv = Map(
                    "DMLC_TASK_ID" -> TaskContext.getPartitionId().toString)
                  Rabit.init(rabitEnv.asJava)

                  converter = new RowConverter(bOrigSchema.value,
                    (0 until table.getNumberOfColumns).map(table.getColumn(_).getType))
                }

                val dataTypes = bOrigSchema.value.fields.map(x => x.dataType)

                val devCb = GpuColumnVector.from(table, dataTypes)

                try {
                  cb = new ColumnarBatch(
                    GpuColumnVector.extractColumns(devCb).map(_.copyToHost()),
                    devCb.numRows())

                  val rowIterator = cb.rowIterator().asScala
                    .map(toUnsafe)
                    .map(converter.toExternalRow(_))

                  val gpuColumnBatch = new GpuColumnBatch(table, bOrigSchema.value)

                  // Create DMatrix
                  val cudfColumnBatch = new CudfColumnBatch(gpuColumnBatch.slice(
                    GpuUtils.seqIntToSeqInteger(featureIds).asJava), null, null, null)
                  val dm = new DMatrix(cudfColumnBatch, missing, 1)
                  it = {
                    if (dm == null) {
                      Iterator.empty
                    } else {
                      try {
                        // set some params of gpu related to booster
                        // - gpu id
                        // - predictor: Force to gpu predictor since native doesn't save predictor.
                        val gpuId = GpuUtils.getGpuId(isLocal)
                        bBooster.value.setParam("gpu_id", gpuId.toString)
                        bBooster.value.setParam("predictor", "gpu_predictor")
                        logger.info("XGBoost transform GPU pipeline using device: " + gpuId)

                        predictFunc(bBooster, dm, rowIterator)
                      } finally {
                        dm.delete()
                      }
                    }
                  }
                } finally {
                  batchCnt += 1
                  devCb.close()
                  table.close()
                }
              }
            }

            override def hasNext: Boolean = {
              val itHasNext = it != null && it.hasNext
              if (!itHasNext) {
                loadNextBatch()
                it != null && it.hasNext
              } else {
                itHasNext
              }
            }

            override def next(): Row = {
              if (it == null || !it.hasNext) {
                loadNextBatch()
              }
              if (it == null) {
                throw new NoSuchElementException()
              }
              it.next()
            }
          }

        } else {
          // should be deprected in some days
          // Init rabit
          val rabitEnv = Map(
            "DMLC_TASK_ID" -> TaskContext.getPartitionId().toString,
            "DMLC_WORKER_STOP_PROCESS_ON_ERROR" -> "false")
          Rabit.init(rabitEnv.asJava)
          try {
            // build DMatrix
            val iterCol = iter.map(t => new GpuColumnBatch(t, bOrigSchema.value,
              sampler.getOrElse(null)))
            val ((dm, cbToRow), time) = MLUtils.time {
              GpuUtils.buildDMatrixAndColumnToRow(missing, iterCol, featureIds, bRowSchema.value)
            }

            if (dm == null) {
              logger.info("No data when building DMatrix")
              Iterator.empty
            } else {
              logger.debug("Benchmark [Transform: Build Dmatrix and Row] " + time)

              try {
                // set some params of gpu related to booster
                // - gpu id
                // - predictor: Force to gpu predictor since native doesn't save predictor.
                val gpuId = GpuUtils.getGpuId(isLocal)
                bBooster.value.setParam("gpu_id", gpuId.toString)
                bBooster.value.setParam("predictor", "gpu_predictor")
                logger.info("XGBoost transform GPU pipeline using device: " + gpuId)

                predictFunc(bBooster, dm, cbToRow.toIterator)
              } finally {
                dm.delete()
              }
            }
          } finally {
            Rabit.shutdown()
          }
        }
    }

    bOrigSchema.unpersist(blocking = false)
    bRowSchema.unpersist(blocking = false)
    bBooster.unpersist(blocking = false)
    ds.sparkSession.createDataFrame(rowRDD, buildRddSchema(rowSchema))
  }

}
